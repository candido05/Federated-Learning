# Federated Learning - Tree-Based Models

ImplementaÃ§Ã£o modular e refatorada de Federated Learning com modelos baseados em Ã¡rvore (XGBoost, LightGBM, CatBoost) usando o framework Flower.

## ğŸ“ Estrutura do Projeto

```
tcc_code/
â”œâ”€â”€ common/                    # ğŸ”§ MÃ³dulos compartilhados
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_processing.py    # Processamento e particionamento do dataset HIGGS
â”‚   â””â”€â”€ metrics_logger.py     # CÃ¡lculo de mÃ©tricas (AUC, F1, etc.) e logging
â”‚
â”œâ”€â”€ algorithms/                # ğŸ¤– ImplementaÃ§Ãµes de FL para cada algoritmo
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ xgboost_fl.py         # Cliente, servidor e execuÃ§Ã£o para XGBoost
â”‚
â”œâ”€â”€ archive/                   # ğŸ“¦ CÃ³digos funcionais originais (PRESERVADOS)
â”‚   â”œâ”€â”€ xgboost.py            # CÃ³digo original XGBoost (funcional em Colab)
â”‚   â”œâ”€â”€ ligthGBM.py           # CÃ³digo original LightGBM (funcional em Colab)
â”‚   â””â”€â”€ catbbost.py           # CÃ³digo original CatBoost (funcional em Colab)
â”‚
â”œâ”€â”€ run_experiments.py         # â–¶ï¸ Script principal de execuÃ§Ã£o
â”œâ”€â”€ requirements.txt           # ğŸ“‹ DependÃªncias do projeto
â””â”€â”€ README.md                  # ğŸ“– Este arquivo
```

## ğŸš€ InstalaÃ§Ã£o

### 1. Criar ambiente virtual

```bash
cd Code/tcc_code
python -m venv venv

# Windows
venv\Scripts\activate

# Linux/Mac
source venv/bin/activate
```

### 2. Instalar dependÃªncias

```bash
pip install -r requirements.txt
```

## ğŸ’» Uso

### ExecuÃ§Ã£o BÃ¡sica

```bash
# XGBoost com estratÃ©gia Cyclic
python run_experiments.py --algorithm xgboost --strategy cyclic

# XGBoost com estratÃ©gia Bagging
python run_experiments.py --algorithm xgboost --strategy bagging

# Executar ambas estratÃ©gias
python run_experiments.py --algorithm xgboost --strategy both
```

### ParÃ¢metros DisponÃ­veis

```bash
python run_experiments.py --help

OpÃ§Ãµes:
  --algorithm {xgboost,lightgbm,catboost,all}
                        Algoritmo a executar (padrÃ£o: xgboost)
  --strategy {cyclic,bagging,both}
                        EstratÃ©gia de agregaÃ§Ã£o (padrÃ£o: cyclic)
  --num-clients NUM     NÃºmero de clientes (padrÃ£o: 6)
  --num-rounds NUM      NÃºmero de rodadas do servidor (padrÃ£o: 6)
  --local-rounds NUM    Rodadas locais de boosting (padrÃ£o: 20)
  --samples NUM         Amostras por cliente (padrÃ£o: 8000)
  --seed NUM            Random seed (padrÃ£o: 42)
```

### Exemplos AvanÃ§ados

```bash
# Experimento customizado - mais clientes e rodadas
python run_experiments.py \
    --algorithm xgboost \
    --strategy both \
    --num-clients 10 \
    --num-rounds 10 \
    --local-rounds 30 \
    --samples 5000

# Teste rÃ¡pido com poucos dados
python run_experiments.py \
    --num-clients 3 \
    --num-rounds 3 \
    --samples 2000
```

## ğŸ“Š MÃ©tricas Coletadas

Para cada rodada, sÃ£o calculadas automaticamente:
- âœ… **AcurÃ¡cia** (Accuracy)
- âœ… **PrecisÃ£o** (Precision)
- âœ… **RevocaÃ§Ã£o** (Recall)
- âœ… **F1-Score**
- âœ… **AUC-ROC**
- âœ… **Especificidade**
- âœ… **Matriz de ConfusÃ£o** (TN, FP, FN, TP)

## ğŸ“ˆ Outputs

### Arquivo de Resultados
ApÃ³s a execuÃ§Ã£o, um arquivo JSON Ã© gerado: `federated_learning_results.json`

```json
{
  "xgboost_cyclic": {
    "metrics_distributed": [...],
    "metrics_centralized": [...],
    "losses_distributed": [...],
    "losses_centralized": [...]
  }
}
```

### Console
MÃ©tricas sÃ£o impressas em tempo real:

```
[Server] Round 1 MÃ©tricas de Performance:
  AcurÃ¡cia:    0.8542
  PrecisÃ£o:    0.8331
  RevocaÃ§Ã£o:   0.8765
  F1-Score:    0.8543
  AUC:         0.9102
  Especific.:  0.8319
  Matriz de ConfusÃ£o:
    TN: 3421 | FP:  679
    FN:  498 | TP: 3402
```

## ğŸ—ï¸ Arquitetura Modular

### 1. `common/data_processing.py`
- **DataProcessor**: Classe para carregar e particionar dataset HIGGS
- **replace_keys()**: UtilitÃ¡rio para converter configuraÃ§Ãµes

### 2. `common/metrics_logger.py`
- **calculate_comprehensive_metrics()**: Calcula todas as mÃ©tricas
- **print_metrics_summary()**: Imprime mÃ©tricas formatadas
- **ExperimentLogger**: Gerencia logging completo de experimentos
- **evaluate_metrics_aggregation()**: Agrega mÃ©tricas dos clientes

### 3. `algorithms/xgboost_fl.py`
- **XGBoostClient**: Cliente FL para XGBoost
- **run_xgboost_experiment()**: FunÃ§Ã£o principal para executar experimento
- Suporte automÃ¡tico para GPU/CPU
- EstratÃ©gias Cyclic e Bagging

## ğŸ¯ EstratÃ©gias de AgregaÃ§Ã£o

### Cyclic (CÃ­clica)
- âš¡ Treina **um cliente por rodada**, sequencialmente
- ğŸ”„ Modelo passa de cliente em cliente
- âœ… Melhor para convergÃªncia gradual
- ğŸ’¾ Menor uso de memÃ³ria

### Bagging
- ğŸš€ **Todos os clientes treinam em paralelo**
- ğŸ”€ Modelos sÃ£o agregados no servidor
- âš¡ Mais rÃ¡pido (processamento paralelo)
- ğŸ’» Requer mais recursos computacionais

## ğŸ“¦ CÃ³digos Originais (Archive)

Os cÃ³digos funcionais originais estÃ£o **preservados** em `archive/`:
- `xgboost.py` - CÃ³digo original do XGBoost (testado e funcional no Colab)
- `ligthGBM.py` - CÃ³digo original do LightGBM (testado e funcional no Colab)
- `catbbost.py` - CÃ³digo original do CatBoost (testado e funcional no Colab)

**âš ï¸ Estes arquivos sÃ£o referÃªncia e NÃƒO devem ser modificados.**

## âœ… Status de ImplementaÃ§Ã£o

- âœ… **XGBoost**: Totalmente funcional e modularizado para VSCode
- â³ **LightGBM**: Em desenvolvimento (use `archive/ligthGBM.py` temporariamente)
- â³ **CatBoost**: Em desenvolvimento (use `archive/catbbost.py` temporariamente)

## ğŸ› Troubleshooting

### Erro: "Module not found"
Certifique-se de executar a partir do diretÃ³rio correto:
```bash
cd Code/tcc_code
python run_experiments.py ...
```

### Erro: "CUDA out of memory"
Reduza o nÃºmero de clientes ou amostras:
```bash
python run_experiments.py --num-clients 4 --samples 5000
```

### Dataset HIGGS nÃ£o baixa
- O dataset Ã© baixado automaticamente do HuggingFace
- Certifique-se de ter conexÃ£o com internet
- Pode demorar na primeira execuÃ§Ã£o (~1GB)

### Import errors
Reinstale as dependÃªncias:
```bash
pip install --upgrade -r requirements.txt
```

## ğŸ”¬ Para Desenvolvedores

### Estrutura de um MÃ³dulo de Algoritmo

Cada algoritmo em `algorithms/` deve implementar:

1. **Cliente FL** (classe que herda de `flwr.client.Client`)
   - `fit()`: Treino local com modelo
   - `evaluate()`: AvaliaÃ§Ã£o local

2. **FunÃ§Ãµes Factory**
   - `create_client_fn()`: Cria funÃ§Ã£o de cliente
   - `create_server_fn()`: Cria funÃ§Ã£o de servidor
   - `get_evaluate_fn()`: Cria funÃ§Ã£o de avaliaÃ§Ã£o centralizada

3. **FunÃ§Ã£o Principal**
   - `run_{algorithm}_experiment()`: Orquestra experimento completo

### Exemplo: Template para Novo Algoritmo

```python
# algorithms/new_algorithm_fl.py

from ..common import DataProcessor, calculate_comprehensive_metrics
from flwr.client import Client, ClientApp
from flwr.common import FitIns, FitRes, EvaluateIns, EvaluateRes

class NewAlgorithmClient(Client):
    def fit(self, ins: FitIns) -> FitRes:
        # Implementar treino local
        pass

    def evaluate(self, ins: EvaluateIns) -> EvaluateRes:
        # Implementar avaliaÃ§Ã£o local
        pass

def run_new_algorithm_experiment(data_processor, num_clients, ...):
    # Implementar lÃ³gica completa do experimento
    pass
```

## ğŸ“š ReferÃªncias

- **Flower Framework**: https://flower.ai/
- **XGBoost**: https://xgboost.readthedocs.io/
- **LightGBM**: https://lightgbm.readthedocs.io/
- **CatBoost**: https://catboost.ai/
- **Dataset HIGGS**: https://huggingface.co/datasets/jxie/higgs

## ğŸ“ LicenÃ§a

Este cÃ³digo faz parte de um projeto de TCC (Trabalho de ConclusÃ£o de Curso) sobre **"Optimization of Federated Learning Models with SDN (Software-Defined Networking)"**.

---

**Desenvolvido com ğŸ¤– para TCC - Federated Learning com SDN**
