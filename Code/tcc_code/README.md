# Federated Learning - Tree-Based Models

ImplementaÃ§Ã£o modular e refatorada de Federated Learning com modelos baseados em Ã¡rvore (XGBoost, LightGBM, CatBoost) usando o framework Flower.

## ğŸ“ Estrutura do Projeto

```
tcc_code/
â”œâ”€â”€ common/                    # ğŸ”§ MÃ³dulos compartilhados
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_processing.py    # Processamento e particionamento do dataset HIGGS
â”‚   â””â”€â”€ metrics_logger.py     # CÃ¡lculo de mÃ©tricas (AUC, F1, etc.) e logging
â”‚
â”œâ”€â”€ algorithms/                # ğŸ¤– ImplementaÃ§Ãµes de FL para cada algoritmo
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ xgboost_fl.py         # Cliente, servidor e execuÃ§Ã£o para XGBoost
â”‚
â”œâ”€â”€ archive/                   # ğŸ“¦ CÃ³digos funcionais originais (PRESERVADOS)
â”‚   â”œâ”€â”€ xgboost.py            # CÃ³digo original XGBoost (funcional em Colab)
â”‚   â”œâ”€â”€ ligthGBM.py           # CÃ³digo original LightGBM (funcional em Colab)
â”‚   â””â”€â”€ catbbost.py           # CÃ³digo original CatBoost (funcional em Colab)
â”‚
â”œâ”€â”€ run_experiments.py         # â–¶ï¸ Script principal de execuÃ§Ã£o
â”œâ”€â”€ requirements.txt           # ğŸ“‹ DependÃªncias do projeto
â””â”€â”€ README.md                  # ğŸ“– Este arquivo
```

## ğŸš€ InstalaÃ§Ã£o

### 1. Criar ambiente virtual

```bash
cd Code/tcc_code
python -m venv venv

# Windows
venv\Scripts\activate

# Linux/Mac
source venv/bin/activate
```

### 2. Instalar dependÃªncias

```bash
pip install -r requirements.txt
```

## ğŸ’» Uso

### âš¡ ExecuÃ§Ã£o RÃ¡pida - Scripts All-in-One

**Para executar todos os experimentos de um algoritmo (Cyclic + Bagging):**

```bash
# CatBoost - Todas as estratÃ©gias
PYTHONPATH=. python run_catboost_all.py

# XGBoost - Todas as estratÃ©gias
PYTHONPATH=. python run_xgboost_all.py

# LightGBM - Todas as estratÃ©gias
PYTHONPATH=. python run_lightgbm_all.py
```

**Para executar TODOS os 6 experimentos (XGBoost, LightGBM, CatBoost Ã— Cyclic, Bagging):**

```bash
PYTHONPATH=. python run_all_experiments.py
```

### ExecuÃ§Ã£o Individual via CLI

```bash
# XGBoost com estratÃ©gia Cyclic
PYTHONPATH=. python run_experiments.py --algorithm xgboost --strategy cyclic

# XGBoost com estratÃ©gia Bagging
PYTHONPATH=. python run_experiments.py --algorithm xgboost --strategy bagging

# Executar ambas estratÃ©gias
PYTHONPATH=. python run_experiments.py --algorithm xgboost --strategy both
```

### ParÃ¢metros DisponÃ­veis

```bash
python run_experiments.py --help

OpÃ§Ãµes:
  --algorithm {xgboost,lightgbm,catboost,all}
                        Algoritmo a executar (padrÃ£o: xgboost)
  --strategy {cyclic,bagging,both}
                        EstratÃ©gia de agregaÃ§Ã£o (padrÃ£o: cyclic)
  --num-clients NUM     NÃºmero de clientes (padrÃ£o: 6)
  --num-rounds NUM      NÃºmero de rodadas do servidor (padrÃ£o: 6)
  --local-rounds NUM    Rodadas locais de boosting (padrÃ£o: 20)
  --samples NUM         Amostras por cliente (padrÃ£o: 8000)
  --seed NUM            Random seed (padrÃ£o: 42)
```

### Exemplos AvanÃ§ados

```bash
# Experimento customizado - mais clientes e rodadas
python run_experiments.py \
    --algorithm xgboost \
    --strategy both \
    --num-clients 10 \
    --num-rounds 10 \
    --local-rounds 30 \
    --samples 5000

# Teste rÃ¡pido com poucos dados
python run_experiments.py \
    --num-clients 3 \
    --num-rounds 3 \
    --samples 2000
```

## ğŸ“Š MÃ©tricas Coletadas

Para cada rodada, sÃ£o calculadas automaticamente:
- âœ… **AcurÃ¡cia** (Accuracy)
- âœ… **PrecisÃ£o** (Precision)
- âœ… **RevocaÃ§Ã£o** (Recall)
- âœ… **F1-Score**
- âœ… **AUC-ROC**
- âœ… **Especificidade**
- âœ… **Matriz de ConfusÃ£o** (TN, FP, FN, TP)

## ğŸ“ˆ Outputs

### Sistema de Logging Estruturado

Todos os experimentos sÃ£o automaticamente salvos na pasta `logs/` organizada por **algoritmo â†’ data/hora â†’ estratÃ©gia**:

```
logs/
â”œâ”€â”€ xgboost/
â”‚   â”œâ”€â”€ 20251021_143052_cyclic/       # Pasta Ãºnica por execuÃ§Ã£o
â”‚   â”‚   â”œâ”€â”€ execution_log.txt         # Log completo com todas as mÃ©tricas
â”‚   â”‚   â”œâ”€â”€ metrics.json              # Dados estruturados em JSON
â”‚   â”‚   â””â”€â”€ README.md                 # Resumo do experimento
â”‚   â””â”€â”€ 20251021_144120_bagging/
â”‚       â”œâ”€â”€ execution_log.txt
â”‚       â”œâ”€â”€ metrics.json
â”‚       â””â”€â”€ README.md
â”œâ”€â”€ lightgbm/
â”‚   â””â”€â”€ ...
â””â”€â”€ catboost/
    â””â”€â”€ ...
```

**Vantagens dessa estrutura:**
- âœ… Cada execuÃ§Ã£o tem sua prÃ³pria pasta com timestamp
- âœ… FÃ¡cil identificar quando o experimento foi executado
- âœ… Nunca sobrescreve resultados anteriores
- âœ… README.md em cada pasta para navegaÃ§Ã£o rÃ¡pida

### Arquivo de Log de Texto (.txt)

ContÃ©m output formatado de cada experimento:

```
================================================================================
INICIANDO EXPERIMENTO: XGBOOST - CYCLIC
================================================================================
ConfiguraÃ§Ã£o:
  - Algoritmo: xgboost
  - EstratÃ©gia: cyclic
  - NÃºmero de clientes: 6
  - Rodadas globais: 3
  - Rodadas locais: 20
  - Amostras por cliente: 2000
================================================================================

[SERVER] Round 1 MÃ©tricas de Performance:
  AcurÃ¡cia:    0.6954
  PrecisÃ£o:    0.6987
  RevocaÃ§Ã£o:   0.6880
  F1-Score:    0.6933
  AUC:         0.7651
  Especific.:  0.7028
  Matriz de ConfusÃ£o:
    TN:  281 | FP:  119
    FN:  125 | TP:  275

...

================================================================================
EXPERIMENTO CONCLUÃDO: XGBOOST - CYCLIC
Tempo total: 45.32 segundos
================================================================================
```

### Arquivo JSON (.json)

ContÃ©m dados estruturados completos:

```json
{
  "experiment_info": {
    "algorithm": "xgboost",
    "strategy": "cyclic",
    "num_clients": 6,
    "num_rounds": 3,
    "num_local_rounds": 20,
    "samples_per_client": 2000,
    "total_time_seconds": 45.32
  },
  "metrics_by_round": {
    "1": {
      "accuracy": 0.6954,
      "precision": 0.6987,
      "recall": 0.6880,
      "f1_score": 0.6933,
      "auc": 0.7651,
      "specificity": 0.7028,
      "confusion_matrix": {
        "tn": 281, "fp": 119,
        "fn": 125, "tp": 275
      }
    },
    ...
  },
  "detailed_logs": [...],
  "final_history": "History(...)"
}
```

### Console
MÃ©tricas sÃ£o impressas em tempo real (e salvas no arquivo .txt):

```
[SERVER] Round 1 MÃ©tricas de Performance:
  AcurÃ¡cia:    0.8542
  PrecisÃ£o:    0.8331
  RevocaÃ§Ã£o:   0.8765
  F1-Score:    0.8543
  AUC:         0.9102
  Especific.:  0.8319
  Matriz de ConfusÃ£o:
    TN: 3421 | FP:  679
    FN:  498 | TP: 3402
```

## ğŸ—ï¸ Arquitetura Modular

### 1. `common/data_processing.py`
- **DataProcessor**: Classe para carregar e particionar dataset HIGGS
- **replace_keys()**: UtilitÃ¡rio para converter configuraÃ§Ãµes

### 2. `common/metrics_logger.py`
- **calculate_comprehensive_metrics()**: Calcula todas as mÃ©tricas (AUC, F1, Precision, Recall, etc.)
- **print_metrics_summary()**: Imprime mÃ©tricas formatadas no console
- **ExperimentLogger**: Gerencia logging completo de experimentos
  - Cria diretÃ³rios automaticamente em `logs/{algorithm}/`
  - Salva logs em tempo real (.txt) e dados estruturados (.json)
  - Rastreia tempo de execuÃ§Ã£o e histÃ³rico completo
  - MÃ©todos principais:
    - `start_experiment()`: Inicializa experimento e logging
    - `log_round_metrics()`: Registra mÃ©tricas de cada rodada
    - `log_aggregated_metrics()`: Registra mÃ©tricas agregadas
    - `end_experiment()`: Finaliza e salva todos os dados
- **evaluate_metrics_aggregation()**: Agrega mÃ©tricas de mÃºltiplos clientes

### 3. `algorithms/xgboost_fl.py`
- **XGBoostClient**: Cliente FL para XGBoost
- **run_xgboost_experiment()**: FunÃ§Ã£o principal para executar experimento
- Suporte automÃ¡tico para GPU/CPU
- EstratÃ©gias Cyclic e Bagging

## ğŸ¯ EstratÃ©gias de AgregaÃ§Ã£o

### Cyclic (CÃ­clica)
- âš¡ Treina **um cliente por rodada**, sequencialmente
- ğŸ”„ Modelo passa de cliente em cliente
- âœ… Melhor para convergÃªncia gradual
- ğŸ’¾ Menor uso de memÃ³ria

### Bagging
- ğŸš€ **Todos os clientes treinam em paralelo**
- ğŸ”€ Modelos sÃ£o agregados no servidor
- âš¡ Mais rÃ¡pido (processamento paralelo)
- ğŸ’» Requer mais recursos computacionais

## ğŸ“¦ CÃ³digos Originais (Archive)

Os cÃ³digos funcionais originais estÃ£o **preservados** em `archive/`:
- `xgboost.py` - CÃ³digo original do XGBoost (testado e funcional no Colab)
- `ligthGBM.py` - CÃ³digo original do LightGBM (testado e funcional no Colab)
- `catbbost.py` - CÃ³digo original do CatBoost (testado e funcional no Colab)

**âš ï¸ Estes arquivos sÃ£o referÃªncia e NÃƒO devem ser modificados.**

## âœ… Status de ImplementaÃ§Ã£o

- âœ… **XGBoost**: Totalmente funcional e modularizado para VSCode
- â³ **LightGBM**: Em desenvolvimento (use `archive/ligthGBM.py` temporariamente)
- â³ **CatBoost**: Em desenvolvimento (use `archive/catbbost.py` temporariamente)

## ğŸ› Troubleshooting

### Erro: "Module not found"
Certifique-se de executar a partir do diretÃ³rio correto:
```bash
cd Code/tcc_code
python run_experiments.py ...
```

### Erro: "CUDA out of memory"
Reduza o nÃºmero de clientes ou amostras:
```bash
python run_experiments.py --num-clients 4 --samples 5000
```

### Dataset HIGGS nÃ£o baixa
- O dataset Ã© baixado automaticamente do HuggingFace
- Certifique-se de ter conexÃ£o com internet
- Pode demorar na primeira execuÃ§Ã£o (~1GB)

### Import errors
Reinstale as dependÃªncias:
```bash
pip install --upgrade -r requirements.txt
```

## ğŸ”¬ Para Desenvolvedores

### Estrutura de um MÃ³dulo de Algoritmo

Cada algoritmo em `algorithms/` deve implementar:

1. **Cliente FL** (classe que herda de `flwr.client.Client`)
   - `fit()`: Treino local com modelo
   - `evaluate()`: AvaliaÃ§Ã£o local

2. **FunÃ§Ãµes Factory**
   - `create_client_fn()`: Cria funÃ§Ã£o de cliente
   - `create_server_fn()`: Cria funÃ§Ã£o de servidor
   - `get_evaluate_fn()`: Cria funÃ§Ã£o de avaliaÃ§Ã£o centralizada

3. **FunÃ§Ã£o Principal**
   - `run_{algorithm}_experiment()`: Orquestra experimento completo

### Exemplo: Template para Novo Algoritmo

```python
# algorithms/new_algorithm_fl.py

from ..common import DataProcessor, calculate_comprehensive_metrics
from flwr.client import Client, ClientApp
from flwr.common import FitIns, FitRes, EvaluateIns, EvaluateRes

class NewAlgorithmClient(Client):
    def fit(self, ins: FitIns) -> FitRes:
        # Implementar treino local
        pass

    def evaluate(self, ins: EvaluateIns) -> EvaluateRes:
        # Implementar avaliaÃ§Ã£o local
        pass

def run_new_algorithm_experiment(data_processor, num_clients, ...):
    # Implementar lÃ³gica completa do experimento
    pass
```

## ğŸ“š ReferÃªncias

- **Flower Framework**: https://flower.ai/
- **XGBoost**: https://xgboost.readthedocs.io/
- **LightGBM**: https://lightgbm.readthedocs.io/
- **CatBoost**: https://catboost.ai/
- **Dataset HIGGS**: https://huggingface.co/datasets/jxie/higgs

## ğŸ“ LicenÃ§a

Este cÃ³digo faz parte de um projeto de TCC (Trabalho de ConclusÃ£o de Curso) sobre **"Optimization of Federated Learning Models with SDN (Software-Defined Networking)"**.

---

**Desenvolvido com ğŸ¤– para TCC - Federated Learning com SDN**
